{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training_templateのノートブック"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch==1.6.0 in /home/user/.local/lib/python3.6/site-packages (1.6.0)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.6.0) (0.18.2)\n",
      "Requirement already satisfied: numpy in /home/user/.local/lib/python3.6/site-packages (from torch==1.6.0) (1.19.2)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.2; however, version 20.2.3 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==1.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py:886: MatplotlibDeprecationWarning: \n",
      "examples.directory is deprecated; in the future, examples will be found relative to the 'datapath' directory.\n",
      "  \"found relative to the 'datapath' directory.\".format(key))\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, gc\n",
    "os.environ[\"L5KIT_DATA_FOLDER\"] = '/home/knikaido/work/Lyft/data/lyft-motion-prediction-autonomous-vehicles'\n",
    "import zarr\n",
    "from prettytable import PrettyTable\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import cloudpickle\n",
    "import requests\n",
    "import mlflow\n",
    "import yaml\n",
    "from typing import Dict\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models.resnet import resnet18, resnet50, resnet34\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "from l5kitcustom.data import PERCEPTION_LABELS\n",
    "from l5kitcustom.rasterization import build_rasterizer\n",
    "from l5kitcustom.configs import load_config_data\n",
    "from l5kitcustom.visualization import draw_trajectory, TARGET_POINTS_COLOR\n",
    "from l5kitcustom.geometry import transform_points\n",
    "from l5kitcustom.data import ChunkedDataset, LocalDataManager\n",
    "from l5kitcustom.dataset import EgoDataset, AgentDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = True\n",
    "\n",
    "common_cfg = {\n",
    "    'seed': 500,\n",
    "    'output_dir': './outputs/1004/',\n",
    "    'epoch': 2,\n",
    "    'train_step': 5 if DEBUG else 500,\n",
    "    'valid_step': 5 if DEBUG else 100,\n",
    "    'train_max': 12,\n",
    "    'learning_rate': 1e-3\n",
    "}\n",
    "\n",
    "\n",
    "# training cfg\n",
    "training_cfg = {\n",
    "    \n",
    "    'format_version': 4,\n",
    "    \n",
    "     ## Model options\n",
    "    'model_params': {\n",
    "        'model_architecture': 'resnet34',\n",
    "        'history_num_frames': 10,\n",
    "        'history_step_size': 1,\n",
    "        'history_delta_time': 0.1,\n",
    "        'future_num_frames': 50,\n",
    "        'future_step_size': 1,\n",
    "        'future_delta_time': 0.1,\n",
    "    },\n",
    "\n",
    "    ## Input raster parameters\n",
    "    'raster_params': {\n",
    "        \n",
    "        'raster_size': [224, 224], # raster's spatial resolution [meters per pixel]: the size in the real world one pixel corresponds to.\n",
    "        'pixel_size': [0.5, 0.5], # From 0 to 1 per axis, [0.5,0.5] would show the ego centered in the image.\n",
    "        'ego_center': [0.25, 0.5],\n",
    "        'map_type': \"py_semantic\",\n",
    "        \n",
    "        # the keys are relative to the dataset environment variable\n",
    "        'satellite_map_key': \"aerial_map/aerial_map.png\",\n",
    "        'semantic_map_key': \"semantic_map/semantic_map.pb\",\n",
    "        'dataset_meta_key': \"meta.json\",\n",
    "\n",
    "        # e.g. 0.0 include every obstacle, 0.5 show those obstacles with >0.5 probability of being\n",
    "        # one of the classes we care about (cars, bikes, peds, etc.), >=1.0 filter all other agents.\n",
    "        'filter_agents_threshold': 0.5\n",
    "    },\n",
    "\n",
    "    ## Data loader options\n",
    "    'train_data_loader': {\n",
    "        'key': \"scenes/train.zarr\",\n",
    "        'batch_size': 32,\n",
    "        'shuffle': True,\n",
    "        'num_workers': 0\n",
    "    },\n",
    "\n",
    "    ## Train params\n",
    "    'train_params': {\n",
    "        'checkpoint_every_n_steps': 5000,\n",
    "        'max_num_steps': 10 if DEBUG else 10000\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "# training cfg\n",
    "validation_cfg = {\n",
    "    \n",
    "    'format_version': 4,\n",
    "    \n",
    "     ## Model options\n",
    "    'model_params': {\n",
    "        'model_architecture': 'resnet34',\n",
    "        'history_num_frames': 10,\n",
    "        'history_step_size': 1,\n",
    "        'history_delta_time': 0.1,\n",
    "        'future_num_frames': 50,\n",
    "        'future_step_size': 1,\n",
    "        'future_delta_time': 0.1,\n",
    "    },\n",
    "\n",
    "    ## Input raster parameters\n",
    "    'raster_params': {\n",
    "        \n",
    "        'raster_size': [224, 224], # raster's spatial resolution [meters per pixel]: the size in the real world one pixel corresponds to.\n",
    "        'pixel_size': [0.5, 0.5], # From 0 to 1 per axis, [0.5,0.5] would show the ego centered in the image.\n",
    "        'ego_center': [0.25, 0.5],\n",
    "        'map_type': \"py_semantic\",\n",
    "        \n",
    "        # the keys are relative to the dataset environment variable\n",
    "        'satellite_map_key': \"aerial_map/aerial_map.png\",\n",
    "        'semantic_map_key': \"semantic_map/semantic_map.pb\",\n",
    "        'dataset_meta_key': \"meta.json\",\n",
    "\n",
    "        # e.g. 0.0 include every obstacle, 0.5 show those obstacles with >0.5 probability of being\n",
    "        # one of the classes we care about (cars, bikes, peds, etc.), >=1.0 filter all other agents.\n",
    "        'filter_agents_threshold': 0.5\n",
    "    },\n",
    "\n",
    "    ## Data loader options\n",
    "    'valid_data_loader': {\n",
    "        'key': \"scenes/validate.zarr\",\n",
    "        'batch_size': 32,\n",
    "        'shuffle': True,\n",
    "        'num_workers': 0\n",
    "    },\n",
    "\n",
    "    ## Valid params\n",
    "    'valid_params': {\n",
    "        'checkpoint_every_n_steps': 5000,\n",
    "        'max_num_steps': 10 if DEBUG else 1000\n",
    "    }\n",
    "}\n",
    "\n",
    "# # inference cfg\n",
    "# inference_cfg = {\n",
    "    \n",
    "#     'format_version': 4,\n",
    "#     'model_params': {\n",
    "#         'history_num_frames': 10,\n",
    "#         'history_step_size': 1,\n",
    "#         'history_delta_time': 0.1,\n",
    "#         'future_num_frames': 50,\n",
    "#         'future_step_size': 1,\n",
    "#         'future_delta_time': 0.1\n",
    "#     },\n",
    "    \n",
    "#     'raster_params': {\n",
    "#         'raster_size': [448, 448],\n",
    "#         'pixel_size': [0.5, 0.5],\n",
    "#         'ego_center': [0.25, 0.5],\n",
    "#         'map_type': 'py_semantic',\n",
    "#         'satellite_map_key': 'aerial_map/aerial_map.png',\n",
    "#         'semantic_map_key': 'semantic_map/semantic_map.pb',\n",
    "#         'dataset_meta_key': 'meta.json',\n",
    "#         'filter_agents_threshold': 0.5\n",
    "#     },\n",
    "    \n",
    "#         'test_data_loader': {\n",
    "#         'key': 'scenes/test.zarr',\n",
    "#         'batch_size': 16,\n",
    "#         'shuffle': False,\n",
    "#         'num_workers': 0\n",
    "#     }\n",
    "\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG==False:\n",
    "    mlflow.start_run(run_name='1004.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = common_cfg['output_dir']\n",
    "INPUT_ROOT = Path('/home/knikaido/work/Lyft/data/')\n",
    "DATA_DIR = INPUT_ROOT / 'lyft-motion-prediction-autonomous-vehicles/'\n",
    "SAMPLE_ZARR = DATA_DIR / 'scenes/sample.zarr'\n",
    "\n",
    "# cfg = load_config_data(str(INPUT_ROOT / \"lyft-config-files/visualisation_config.yaml\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 42):\n",
    "#     random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)  # type: ignore\n",
    "#     torch.backends.cudnn.deterministic = True  # type: ignore\n",
    "#     torch.backends.cudnn.benchmark = True  # type: ignore\n",
    "\n",
    "set_seed(common_cfg['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'format_version': 4, 'model_params': {'model_architecture': 'resnet34', 'history_num_frames': 10, 'history_step_size': 1, 'history_delta_time': 0.1, 'future_num_frames': 50, 'future_step_size': 1, 'future_delta_time': 0.1}, 'raster_params': {'raster_size': [224, 224], 'pixel_size': [0.5, 0.5], 'ego_center': [0.25, 0.5], 'map_type': 'py_semantic', 'satellite_map_key': 'aerial_map/aerial_map.png', 'semantic_map_key': 'semantic_map/semantic_map.pb', 'dataset_meta_key': 'meta.json', 'filter_agents_threshold': 0.5}, 'train_data_loader': {'key': 'scenes/train.zarr', 'batch_size': 32, 'shuffle': True, 'num_workers': 0}, 'train_params': {'checkpoint_every_n_steps': 5000, 'max_num_steps': 10}}\n"
     ]
    }
   ],
   "source": [
    "# root directory\n",
    "# DIR_INPUT = \"/kaggle/input/lyft-motion-prediction-autonomous-vehicles\"\n",
    "\n",
    "#submission\n",
    "SINGLE_MODE_SUBMISSION = f\"{DATA_DIR}/single_mode_sample_submission.csv\"\n",
    "MULTI_MODE_SUBMISSION = f\"{DATA_DIR}/multi_mode_sample_submission.csv\"\n",
    "\n",
    "# set env variable for data\n",
    "os.environ[\"L5KIT_DATA_FOLDER\"] = str(DATA_DIR)\n",
    "dm = LocalDataManager(None)\n",
    "print(training_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n",
      "| Num Scenes | Num Frames | Num Agents | Num TR lights | Total Time (hr) | Avg Frames per Scene | Avg Agents per Frame | Avg Scene Time (sec) | Avg Frame frequency |\n",
      "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n",
      "|   16265    |  4039527   | 320124624  |    38735988   |      112.19     |        248.36        |        79.25         |        24.83         |        10.00        |\n",
      "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n"
     ]
    }
   ],
   "source": [
    "# training cfg\n",
    "train_cfg = training_cfg[\"train_data_loader\"]\n",
    "\n",
    "# rasterizer\n",
    "rasterizer = build_rasterizer(training_cfg, dm)\n",
    "\n",
    "# dataloader\n",
    "train_zarr = ChunkedDataset(dm.require(train_cfg[\"key\"])).open()\n",
    "train_dataset = AgentDataset(training_cfg, train_zarr, rasterizer)\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=train_cfg[\"shuffle\"], batch_size=train_cfg[\"batch_size\"], \n",
    "                             num_workers=train_cfg[\"num_workers\"])\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n",
      "| Num Scenes | Num Frames | Num Agents | Num TR lights | Total Time (hr) | Avg Frames per Scene | Avg Agents per Frame | Avg Scene Time (sec) | Avg Frame frequency |\n",
      "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n",
      "|   16220    |  4030296   | 312617887  |    29277930   |      111.97     |        248.48        |        77.57         |        24.85         |        10.00        |\n",
      "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n"
     ]
    }
   ],
   "source": [
    "# validation cfg\n",
    "valid_cfg = validation_cfg[\"valid_data_loader\"]\n",
    "\n",
    "# rasterizer\n",
    "rasterizer = build_rasterizer(validation_cfg, dm)\n",
    "\n",
    "# dataloader\n",
    "valid_zarr = ChunkedDataset(dm.require(valid_cfg[\"key\"])).open()\n",
    "valid_dataset = AgentDataset(validation_cfg, valid_zarr, rasterizer)\n",
    "valid_dataloader = DataLoader(train_dataset, shuffle=valid_cfg[\"shuffle\"], batch_size=valid_cfg[\"batch_size\"], \n",
    "                             num_workers=valid_cfg[\"num_workers\"])\n",
    "print(valid_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pytorch_neg_multi_log_likelihood_batch(\n",
    "    gt: Tensor, pred: Tensor, confidences: Tensor, avails: Tensor\n",
    ") -> Tensor:\n",
    "    \"\"\"\n",
    "    Compute a negative log-likelihood for the multi-modal scenario.\n",
    "    log-sum-exp trick is used here to avoid underflow and overflow, For more information about it see:\n",
    "    https://en.wikipedia.org/wiki/LogSumExp#log-sum-exp_trick_for_log-domain_calculations\n",
    "    https://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/\n",
    "    https://leimao.github.io/blog/LogSumExp/\n",
    "    Args:\n",
    "        gt (Tensor): array of shape (bs)x(time)x(2D coords)\n",
    "        pred (Tensor): array of shape (bs)x(modes)x(time)x(2D coords)\n",
    "        confidences (Tensor): array of shape (bs)x(modes) with a confidence for each mode in each sample\n",
    "        avails (Tensor): array of shape (bs)x(time) with the availability for each gt timestep\n",
    "    Returns:\n",
    "        Tensor: negative log-likelihood for this example, a single float number\n",
    "    \"\"\"\n",
    "    assert len(pred.shape) == 4, f\"expected 3D (MxTxC) array for pred, got {pred.shape}\"\n",
    "    batch_size, num_modes, future_len, num_coords = pred.shape\n",
    "\n",
    "    assert gt.shape == (batch_size, future_len, num_coords), f\"expected 2D (Time x Coords) array for gt, got {gt.shape}\"\n",
    "    assert confidences.shape == (batch_size, num_modes), f\"expected 1D (Modes) array for gt, got {confidences.shape}\"\n",
    "    assert torch.allclose(torch.sum(confidences, dim=1), confidences.new_ones((batch_size,))), \"confidences should sum to 1\"\n",
    "    assert avails.shape == (batch_size, future_len), f\"expected 1D (Time) array for gt, got {avails.shape}\"\n",
    "    # assert all data are valid\n",
    "    assert torch.isfinite(pred).all(), \"invalid value found in pred\"\n",
    "    assert torch.isfinite(gt).all(), \"invalid value found in gt\"\n",
    "    assert torch.isfinite(confidences).all(), \"invalid value found in confidences\"\n",
    "    assert torch.isfinite(avails).all(), \"invalid value found in avails\"\n",
    "\n",
    "    # convert to (batch_size, num_modes, future_len, num_coords)\n",
    "    gt = torch.unsqueeze(gt, 1)  # add modes\n",
    "    avails = avails[:, None, :, None]  # add modes and cords\n",
    "\n",
    "    # error (batch_size, num_modes, future_len)\n",
    "    error = torch.sum(((gt - pred) * avails) ** 2, dim=-1)  # reduce coords and use availability\n",
    "\n",
    "    with np.errstate(divide=\"ignore\"):  # when confidence is 0 log goes to -inf, but we're fine with it\n",
    "        # error (batch_size, num_modes)\n",
    "        error = torch.log(confidences) - 0.5 * torch.sum(error, dim=-1)  # reduce time\n",
    "\n",
    "    # use max aggregator on modes for numerical stability\n",
    "    # error (batch_size, num_modes)\n",
    "    max_value, _ = error.max(dim=1, keepdim=True)  # error are negative at this point, so max() gives the minimum one\n",
    "    error = -torch.log(torch.sum(torch.exp(error - max_value), dim=-1, keepdim=True)) - max_value  # reduce modes\n",
    "    # print(\"error\", error)\n",
    "    return torch.mean(error)\n",
    "\n",
    "\n",
    "def pytorch_neg_multi_log_likelihood_single(\n",
    "    gt: Tensor, pred: Tensor, avails: Tensor\n",
    ") -> Tensor:\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        gt (Tensor): array of shape (bs)x(time)x(2D coords)\n",
    "        pred (Tensor): array of shape (bs)x(time)x(2D coords)\n",
    "        avails (Tensor): array of shape (bs)x(time) with the availability for each gt timestep\n",
    "    Returns:\n",
    "        Tensor: negative log-likelihood for this example, a single float number\n",
    "    \"\"\"\n",
    "    # pred (bs)x(time)x(2D coords) --> (bs)x(mode=1)x(time)x(2D coords)\n",
    "    # create confidence (bs)x(mode=1)\n",
    "    batch_size, future_len, num_coords = pred.shape\n",
    "    confidences = pred.new_ones((batch_size, 1))\n",
    "    return pytorch_neg_multi_log_likelihood_batch(gt, pred.unsqueeze(1), confidences, avails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LyftMultiModel(nn.Module):\n",
    "\n",
    "    def __init__(self, cfg: Dict, num_modes=3):\n",
    "        super().__init__()\n",
    "\n",
    "        # TODO: support other than resnet18?\n",
    "        backbone = resnet34(pretrained=True, progress=True)\n",
    "        self.backbone = backbone\n",
    "\n",
    "        num_history_channels = (cfg[\"model_params\"][\"history_num_frames\"] + 1) * 2\n",
    "        num_in_channels = num_history_channels\n",
    "\n",
    "        self.backbone.conv1 = nn.Conv2d(\n",
    "            num_in_channels,\n",
    "            self.backbone.conv1.out_channels,\n",
    "            kernel_size=self.backbone.conv1.kernel_size,\n",
    "            stride=self.backbone.conv1.stride,\n",
    "            padding=self.backbone.conv1.padding,\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "        # This is 512 for resnet18 and resnet34;\n",
    "        # And it is 2048 for the other resnets\n",
    "        backbone_out_features = 512\n",
    "\n",
    "        # X, Y coords for the future positions (output shape: Bx50x2)\n",
    "        self.future_len = cfg[\"model_params\"][\"future_num_frames\"]\n",
    "        num_targets = 2 * self.future_len\n",
    "\n",
    "        # You can add more layers here.\n",
    "        self.head = nn.Sequential(\n",
    "            # nn.Dropout(0.2),\n",
    "            nn.Linear(in_features=backbone_out_features, out_features=4096),\n",
    "        )\n",
    "\n",
    "        self.num_preds = num_targets * num_modes\n",
    "        self.num_modes = num_modes\n",
    "\n",
    "        self.logit = nn.Linear(4096, out_features=self.num_preds + num_modes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone.conv1(x)\n",
    "        x = self.backbone.bn1(x)\n",
    "        x = self.backbone.relu(x)\n",
    "        x = self.backbone.maxpool(x)\n",
    "\n",
    "        x = self.backbone.layer1(x)\n",
    "        x = self.backbone.layer2(x)\n",
    "        x = self.backbone.layer3(x)\n",
    "        x = self.backbone.layer4(x)\n",
    "\n",
    "        x = self.backbone.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        x = self.head(x)\n",
    "        x = self.logit(x)\n",
    "\n",
    "        # pred (bs)x(modes)x(time)x(2D coords)\n",
    "        # confidences (bs)x(modes)\n",
    "        bs, _ = x.shape\n",
    "        pred, confidences = torch.split(x, self.num_preds, dim=1)\n",
    "        pred = pred.view(bs, self.num_modes, self.future_len, 2)\n",
    "        assert confidences.shape == (bs, self.num_modes)\n",
    "        confidences = torch.softmax(confidences, dim=1)\n",
    "        return pred, confidences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(data, model, device, criterion = pytorch_neg_multi_log_likelihood_batch):\n",
    "    inputs = data[\"image\"].to(device)\n",
    "    target_availabilities = data[\"target_availabilities\"].to(device)\n",
    "    targets = data[\"target_positions\"].to(device)\n",
    "    # Forward pass\n",
    "    preds, confidences = model(inputs)\n",
    "    loss = criterion(targets, preds, confidences, target_availabilities)\n",
    "    return loss, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling model\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LyftMultiModel(training_cfg).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=common_cfg['learning_rate'])\n",
    "criterion = nn.MSELoss(reduction=\"none\")\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(\n",
    "    optimizer, gamma=0.99999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.6/site-packages/l5kitcustom/dataset/agent.py:115: RuntimeWarning: disable_traffic_light_faces not found in config, this will raise an error in the future\n",
      "  return self.get_frame(scene_index, state_index, track_id=track_id)\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, data in enumerate(train_dataloader):\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 22, 224, 224])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['image'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "703023"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG==False:\n",
    "    common_cfg['train_max'] = len(train_dataloader)/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model):\n",
    "    torch.save(model.state_dict(), OUTPUT_DIR + 'model')\n",
    "    if DEBUG==False:\n",
    "        mlflow.log_artifact( OUTPUT_DIR + 'model')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(message):\n",
    "    print(message)\n",
    "    with open(OUTPUT_DIR + 'log.txt', 'a+') as logger:\n",
    "        logger.write(f'{message}\\n')\n",
    "    if DEBUG==False:\n",
    "        mlflow.log_artifact( OUTPUT_DIR + 'log.txt')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(t_loss, v_loss):\n",
    "    fig = plt.figure(figsize=(4,3), dpi= 80)\n",
    "    plt.title('loss')\n",
    "    plt.plot(t_loss, color='tab:red', label='train', marker='x')\n",
    "    plt.plot(v_loss, color='tab:blue', label='valid', marker='x')\n",
    "    plt.minorticks_on()\n",
    "    plt.grid(b=True, which='major', color='#666666', linestyle='-')\n",
    "    plt.grid(b=True, which='minor', color='#999999', linestyle='-', alpha=0.2)\n",
    "    plt.legend()\n",
    "    fig.savefig(OUTPUT_DIR + \"loss.png\")\n",
    "    if DEBUG==False:\n",
    "        mlflow.log_artifact(OUTPUT_DIR + \"loss.png\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def valid_func():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_roop...100epoch_time:589.3819541931152[sec]\n",
      "epoch [1/2] batch_index [499]\n",
      "train_loss = 441.2538358535767, valid_loss = 191.50945950498675\n",
      "save_model\n",
      "valid_roop...100epoch_time:586.082686662674[sec]\n",
      "epoch [1/2] batch_index [999]\n",
      "train_loss = 185.72497451019288, valid_loss = 152.83969157757144\n",
      "save_model\n",
      "train_roop...113"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "best_valid_loss = 100000\n",
    "epoch = common_cfg['epoch']\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "for i in range(epoch):\n",
    "\n",
    "    losses_train = 0\n",
    "    losses_valid = 0\n",
    "    epoch_start = time.time()\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    \n",
    "    \n",
    "    step_train_loss = 0\n",
    "    step_count = 0\n",
    "    for batch_idx, data in enumerate(train_dataloader):\n",
    "        step_count += 1\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            loss, preds = forward(data, model, device)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        step_train_loss += loss.item()\n",
    "        print(\"\\r\"+'train_roop...'+str(step_count),end=\"\")\n",
    "        if step_count == common_cfg['train_step']:\n",
    "            \n",
    "            step_train_loss /= step_count\n",
    "            scheduler.step()\n",
    "            model.eval()\n",
    "            step_valid_loss = 0\n",
    "            for valid_idx, data in enumerate(valid_dataloader):\n",
    "                with torch.no_grad():\n",
    "                    loss, _ = forward(data, model, device)\n",
    "                step_valid_loss += loss.item()\n",
    "                print(\"\\r\"+'valid_roop...'+str(valid_idx),end=\"\")\n",
    "                if valid_idx == common_cfg['valid_step']:\n",
    "                    break  \n",
    "            \n",
    "            step_valid_loss = step_valid_loss / (valid_idx + 1)\n",
    "            epoch_end = time.time() - epoch_start\n",
    "            log(\"epoch_time:{0}\".format(epoch_end) + \"[sec]\")\n",
    "            log(f'epoch [{i+1}/{epoch}] batch_index [{batch_idx}]')\n",
    "            log(f'train_loss = {step_train_loss}, valid_loss = {step_valid_loss}')\n",
    "            if(step_valid_loss < best_valid_loss):\n",
    "                print('save_model')\n",
    "                save_model(model)\n",
    "                best_valid_loss = step_valid_loss\n",
    "            \n",
    "            step_train_loss = 0\n",
    "            step_valid_loss = 0\n",
    "            step_count = 0\n",
    "            epoch_start = time.time()\n",
    "            \n",
    "        if batch_idx >= common_cfg['train_max']:\n",
    "            break\n",
    "            \n",
    "    \n",
    "#     epoch_train_loss = losses_train / (batch_idx + 1)\n",
    "#     train_losses.append(epoch_train_loss)\n",
    "#     print('')\n",
    "    \n",
    "#     model.eval()\n",
    "#     for batch_idx, data in enumerate(valid_dataloader):\n",
    "#         with torch.no_grad():\n",
    "#             loss, _ = forward(data, model, device)\n",
    "#         losses_valid += loss.item()\n",
    "#         print(\"\\r\"+'valid_roop...'+str(batch_idx),end=\"\")\n",
    "#         if batch_idx == validation_cfg[\"valid_params\"][\"max_num_steps\"]:\n",
    "#             break   \n",
    "            \n",
    "#     epoch_end = time.time() - epoch_start\n",
    "#     epoch_valid_loss = losses_valid / (batch_idx + 1)\n",
    "#     valid_losses.append(epoch_valid_loss)\n",
    "#     print('')\n",
    "#     print(\"epoch_time:{0}\".format(epoch_end) + \"[sec]\")\n",
    "#     log(f'epoch [{i+1}/{epoch}] train_loss = {epoch_train_loss}, valid_loss = {epoch_valid_loss}')\n",
    "#     plot_loss(train_losses, valid_losses)\n",
    "    \n",
    "#     if(epoch_valid_loss < best_valid_loss):\n",
    "#         print('save_model')\n",
    "#         save_model(model)\n",
    "#         best_valid_loss = epoch_valid_loss\n",
    "        \n",
    "        \n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_notify_token = 'MZuX1mRyyi4hYt9v1oBSFj5A7Tr7GfMwZnu65DeHJuH'\n",
    "line_notify_api = 'https://notify-api.line.me/api/notify'\n",
    "notification_message = 'おわった'\n",
    "headers = {'Authorization': f'Bearer {line_notify_token}'}\n",
    "data = {'message': f'message: {notification_message}'}\n",
    "requests.post(line_notify_api, headers = headers, data = data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG==False:\n",
    "\n",
    "    with open(OUTPUT_DIR+'common_cfg.yaml', 'w') as file:\n",
    "        yaml.dump(common_cfg, file)\n",
    "    with open(OUTPUT_DIR+'training_cfg.yaml', 'w') as file:\n",
    "        yaml.dump(training_cfg, file)\n",
    "    mlflow.log_artifact(OUTPUT_DIR+'training_cfg.yaml')\n",
    "    with open(OUTPUT_DIR+'validation_cfg.yaml', 'w') as file:\n",
    "        yaml.dump(validation_cfg, file)\n",
    "    mlflow.log_artifact(OUTPUT_DIR+'validation_cfg.yaml')   \n",
    "#     with open(OUTPUT_DIR+'inference_cfg.yaml', 'w') as file:\n",
    "#         yaml.dump(inference_cfg, file)\n",
    "#     mlflow.log_artifact(OUTPUT_DIR+'inference_cfg.yaml')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG==False:\n",
    "    mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# cfg[\"raster_params\"][\"map_type\"] = \"py_semantic\"\n",
    "\n",
    "# # raster object for visualization\n",
    "# rast = build_rasterizer(cfg, dm)\n",
    "\n",
    "# # EgoDataset object\n",
    "# dataset = EgoDataset(cfg, zarr_dataset, rast)\n",
    "\n",
    "# select one example from our dataset\n",
    "data = train_dataset[1]\n",
    "\n",
    "im = data[\"image\"].transpose(1, 2, 0)\n",
    "im = train_dataset.rasterizer.to_rgb(im)\n",
    "target_positions_pixels = transform_points(data[\"target_positions\"] + data[\"centroid\"][:2], data[\"world_to_image\"])\n",
    "\n",
    "# plot ground truth trajectory\n",
    "draw_trajectory(im, target_positions_pixels, TARGET_POINTS_COLOR)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = 10, 10\n",
    "plt.title('Ground Truth Trajectory of Autonomous Vehicle',fontsize=15)\n",
    "plt.imshow(im[::-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['image', 'target_positions', 'target_yaws', 'target_availabilities', 'history_positions', 'history_yaws', 'history_availabilities', 'world_to_image', 'raster_from_world', 'raster_from_agent', 'agent_from_world', 'world_from_agent', 'track_id', 'timestamp', 'centroid', 'yaw', 'extent'])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "423592/22496709"
     ]
    }
   ],
   "source": [
    "directions = [[0, 0], [0, 0]]\n",
    "\n",
    "for i in range(len(train_dataset)):\n",
    "    data = train_dataset[i]\n",
    "    if(data['history_positions'][-1,0] - data['history_positions'][0,0] >= 0):\n",
    "        if(data['history_positions'][-1,1] - data['history_positions'][0,1] >= 0):\n",
    "            directions[0][0] += 1\n",
    "        if(data['history_positions'][-1,1] - data['history_positions'][0,1] < 0):\n",
    "            directions[0][1] += 1\n",
    "    elif(data['history_positions'][-1,0] < data['history_positions'][0,0] < 0):\n",
    "        if(data['history_positions'][-1,1] - data['history_positions'][0,1] >= 0):\n",
    "            directions[1][0] += 1\n",
    "        if(data['history_positions'][-1,1] - data['history_positions'][0,1] < 0):\n",
    "            directions[1][1] += 1\n",
    "    print(\"\\r\"+str(i)+'/'+str(len(train_dataset)),end=\"\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'directions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0a5ae5352127>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdirections\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'directions' is not defined"
     ]
    }
   ],
   "source": [
    "directions[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f31ff6b55c0>]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(data['history_positions'][:,0], data['history_positions'][:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

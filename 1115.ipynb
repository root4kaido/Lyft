{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "from tempfile import gettempdir\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models.resnet import resnet34\n",
    "from tqdm import tqdm\n",
    "\n",
    "from l5kit.configs import load_config_data\n",
    "from l5kit.data import LocalDataManager, ChunkedDataset\n",
    "from l5kit.dataset import AgentDataset, EgoDataset\n",
    "from l5kit.rasterization import build_rasterizer\n",
    "from l5kit.evaluation import write_pred_csv, compute_metrics_csv, read_gt_csv, create_chopped_dataset, read_pred_csv\n",
    "from l5kit.evaluation.chop_dataset import MIN_FUTURE_STEPS\n",
    "from l5kit.evaluation.metrics import neg_multi_log_likelihood, time_displace\n",
    "from l5kit.geometry import transform_points\n",
    "from l5kit.visualization import PREDICTED_POINTS_COLOR, TARGET_POINTS_COLOR, draw_trajectory\n",
    "from prettytable import PrettyTable\n",
    "from pathlib import Path\n",
    "\n",
    "from torch import Tensor\n",
    "from collections import OrderedDict, defaultdict\n",
    "\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data path and load cfg\n",
    "\n",
    "By setting the `L5KIT_DATA_FOLDER` variable, we can point the script to the folder where the data lies.\n",
    "\n",
    "Then, we load our config file with relative paths and other configurations (rasteriser, training params...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "\n",
    "# training cfg\n",
    "validation_cfg = {\n",
    "    \n",
    "    'format_version': 4,\n",
    "    \n",
    "     ## Model options\n",
    "    'model_params': {\n",
    "        'model_architecture': 'resnet34',\n",
    "        'history_num_frames': 10,\n",
    "        'history_step_size': 1,\n",
    "        'history_delta_time': 0.1,\n",
    "        'future_num_frames': 50,\n",
    "        'future_step_size': 1,\n",
    "        'future_delta_time': 0.1,\n",
    "    },\n",
    "\n",
    "    ## Input raster parameters\n",
    "    'raster_params': {\n",
    "        \n",
    "        'raster_size': [224, 224], # raster's spatial resolution [meters per pixel]: the size in the real world one pixel corresponds to.\n",
    "        'pixel_size': [0.5, 0.5], # From 0 to 1 per axis, [0.5,0.5] would show the ego centered in the image.\n",
    "        'ego_center': [0.25, 0.5],\n",
    "        'map_type': \"py_semantic\",\n",
    "        \n",
    "        # the keys are relative to the dataset environment variable\n",
    "        'satellite_map_key': \"aerial_map/aerial_map.png\",\n",
    "        'semantic_map_key': \"semantic_map/semantic_map.pb\",\n",
    "        'dataset_meta_key': \"meta.json\",\n",
    "\n",
    "        # e.g. 0.0 include every obstacle, 0.5 show those obstacles with >0.5 probability of being\n",
    "        # one of the classes we care about (cars, bikes, peds, etc.), >=1.0 filter all other agents.\n",
    "        'filter_agents_threshold': 0.5\n",
    "    },\n",
    "\n",
    "    ## Data loader options\n",
    "    'valid_data_loader': {\n",
    "        'key': \"scenes/validate_chopped_100/validate.zarr\",\n",
    "        'batch_size': 1,\n",
    "        'shuffle': False,\n",
    "        'num_workers': 0\n",
    "    },\n",
    "\n",
    "    ## Valid params\n",
    "    'valid_params': {\n",
    "        'checkpoint_every_n_steps': 5000,\n",
    "        'max_num_steps': 10 if DEBUG else 1000\n",
    "    }\n",
    "}\n",
    "\n",
    "common_cfg = {\n",
    "    'seed': 500,\n",
    "    'output_dir': './outputs/1011_2/',\n",
    "    'epoch': 2,\n",
    "    'train_step': 5 if DEBUG else 500,\n",
    "    'valid_step': 5 if DEBUG else 50,\n",
    "    'train_max': 12,\n",
    "    'learning_rate': 1e-3\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = common_cfg['output_dir']\n",
    "INPUT_ROOT = Path('/home/knikaido/work/Lyft/data/')\n",
    "DATA_DIR = INPUT_ROOT / 'lyft-motion-prediction-autonomous-vehicles/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_path = f\"{OUTPUT_DIR}pred_1114_3.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_gt_path = str(f\"{str(DATA_DIR)}/scenes/validate_chopped_100/gt.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics = compute_metrics_csv(eval_gt_path, pred_path, [neg_multi_log_likelihood, time_displace])\n",
    "# for metric_name, metric_mean in metrics.items():\n",
    "#     print(metric_name, metric_mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_dicts(ground_truth: dict, predicted: dict) -> bool:\n",
    "    \"\"\"\n",
    "    Validate GT and pred dictionaries by comparing keys\n",
    "\n",
    "    Args:\n",
    "        ground_truth (dict): mapping from (track_id + timestamp) to an element returned from our csv utils\n",
    "        predicted (dict): mapping from (track_id + timestamp) to an element returned from our csv utils\n",
    "\n",
    "    Returns:\n",
    "        (bool): True if the 2 dicts match (same keys)\n",
    "\n",
    "    \"\"\"\n",
    "    valid = True\n",
    "\n",
    "    num_agents_gt = len(ground_truth)\n",
    "    num_agents_pred = len(predicted)\n",
    "\n",
    "    if num_agents_gt != num_agents_pred:\n",
    "        print(f\"Incorrect number of rows in inference csv. Expected {num_agents_gt}, Got {num_agents_pred}\")\n",
    "        valid = False\n",
    "\n",
    "    missing_agents = ground_truth.keys() - predicted.keys()\n",
    "    if len(missing_agents):\n",
    "        valid = False\n",
    "\n",
    "    for missing_agents in missing_agents:\n",
    "        print(f\"Missing agents: {missing_agents}\")\n",
    "\n",
    "    unknown_agents = predicted.keys() - ground_truth.keys()\n",
    "    if len(unknown_agents):\n",
    "        valid = False\n",
    "\n",
    "    for unknown_agent in unknown_agents:\n",
    "        print(f\"Unknown agents: {unknown_agent}\")\n",
    "\n",
    "    return valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_csv_(ground_truth_path: str, inference_output_path: str, metrics) -> dict:\n",
    "    \"\"\"\n",
    "    Compute a set of metrics between ground truth and prediction csv files\n",
    "\n",
    "    Arguments:\n",
    "        ground_truth_path (str): Path to the ground truth csv file.\n",
    "        inference_output_path (str): Path to the csv file containing network output.\n",
    "        metrics (List[Callable]): a list of callable to be applied to the elements retrieved from the 2\n",
    "        csv files\n",
    "\n",
    "    Returns:\n",
    "        dict: keys are metrics name, values is the average metric computed over the elements\n",
    "    \"\"\"\n",
    "\n",
    "    assert len(metrics) > 0, \"you must pass at least one metric to compute\"\n",
    "\n",
    "    ground_truth = OrderedDict()\n",
    "    inference = OrderedDict()\n",
    "\n",
    "    for el in read_gt_csv(ground_truth_path):\n",
    "        ground_truth[el[\"track_id\"] + el[\"timestamp\"]] = el\n",
    "    for el in read_pred_csv(inference_output_path):\n",
    "        inference[el[\"track_id\"] + el[\"timestamp\"]] = el\n",
    "\n",
    "    if not validate_dicts(ground_truth, inference):\n",
    "        raise ValueError(\"Error validating csv, see above for details.\")\n",
    "\n",
    "    metrics_dict = defaultdict(list)\n",
    "\n",
    "    for key, ground_truth_value in ground_truth.items():\n",
    "        gt_coord = ground_truth_value[\"coord\"]\n",
    "        avail = ground_truth_value[\"avail\"]\n",
    "\n",
    "        pred_coords = inference[key][\"coords\"]\n",
    "        conf = inference[key][\"conf\"]\n",
    "        for metric in metrics:\n",
    "            metrics_dict[metric.__name__].append(metric(gt_coord, pred_coords, conf, avail))\n",
    "\n",
    "    # compute average of each metric\n",
    "    return {metric_name: np.mean(values, axis=0) for metric_name, values in metrics_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics = compute_metrics_csv_(eval_gt_path, pred_path, [neg_multi_log_likelihood, time_displace])\n",
    "# for metric_name, metric_mean in metrics.items():\n",
    "#     print(metric_name, metric_mean)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = OrderedDict()\n",
    "inference = OrderedDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator, List, Optional\n",
    "import csv\n",
    "from itertools import chain\n",
    "\n",
    "MAX_MODES = 7\n",
    "def read_pred_csv_(csv_path: str) -> Iterator[dict]:\n",
    "    \"\"\"\n",
    "    Generator function that returns a line at the time from the csv file as a dict\n",
    "\n",
    "    Args:\n",
    "        csv_path (str): path of the csv to read\n",
    "\n",
    "    Returns:\n",
    "        Iterator[dict]: dict keys are the csv header fieldnames\n",
    "    \"\"\"\n",
    "\n",
    "    reader = csv.DictReader(open(csv_path, \"r\"))\n",
    "    fieldnames = reader.fieldnames\n",
    "    assert fieldnames is not None, \"error reading fieldnames\"\n",
    "\n",
    "    # exclude timestamp, track_id and MAX_MODES confs, the rest should be (x, y) * len * 3 = 6*len\n",
    "    future_len = (len(fieldnames) - (2 + 7)) / 14\n",
    "    print(future_len)\n",
    "    assert future_len == int(future_len), \"error estimating len\"\n",
    "    future_len = int(future_len)\n",
    "\n",
    "    coords_labels_list = [_generate_coords_keys(future_len, mode_index=idx) for idx in range(MAX_MODES)]\n",
    "    confs_labels = _generate_confs_keys()\n",
    "\n",
    "    for row in reader:\n",
    "        track_id = row[\"track_id\"]\n",
    "        timestamp = row[\"timestamp\"]\n",
    "\n",
    "        conf = np.asarray([np.float64(row[conf_label]) for conf_label in confs_labels])\n",
    "\n",
    "        coords = []\n",
    "        for idx in range(MAX_MODES):\n",
    "            coord = np.asarray([np.float64(row[coord_label]) for coord_label in coords_labels_list[idx]])\n",
    "            coords.append(coord.reshape((future_len, 2)))\n",
    "\n",
    "        coords = np.stack(coords, axis=0)\n",
    "\n",
    "        yield {\"track_id\": track_id, \"timestamp\": timestamp, \"coords\": coords, \"conf\": conf}\n",
    "        \n",
    "def _generate_coords_keys(future_len: int, mode_index: int = 0) -> List[str]:\n",
    "    \"\"\"\n",
    "    Generate keys like coord_x00, coord_y00... that can be used to get or set value in CSV.\n",
    "    Two keys for each mode and future step.\n",
    "\n",
    "    Args:\n",
    "        future_len (int): how many prediction the data has in the future\n",
    "        mode_index (int): what mode are we reading/writing\n",
    "\n",
    "    Returns:\n",
    "        List[str]: a list of keys\n",
    "    \"\"\"\n",
    "    return list(\n",
    "        chain.from_iterable([[f\"coord_x{mode_index}{i}\", f\"coord_y{mode_index}{i}\"] for i in range(future_len)])\n",
    "    )\n",
    "\n",
    "def _generate_confs_keys() -> List[str]:\n",
    "    \"\"\"\n",
    "    Generate modes keys (one per mode)\n",
    "\n",
    "    Returns:\n",
    "        List[str]: a list of keys\n",
    "    \"\"\"\n",
    "    return [f\"conf_{i}\" for i in range(MAX_MODES)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for el in read_gt_csv(eval_gt_path):\n",
    "    ground_truth[el[\"track_id\"] + el[\"timestamp\"]] = el"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.0\n"
     ]
    }
   ],
   "source": [
    "for el in read_pred_csv_(pred_path):\n",
    "    inference[el[\"track_id\"] + el[\"timestamp\"]] = el"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dict = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "errs = []\n",
    "for key, ground_truth_value in ground_truth.items():\n",
    "    gt_coord = ground_truth_value[\"coord\"]\n",
    "    avail = ground_truth_value[\"avail\"]\n",
    "\n",
    "    pred_coords = inference[key][\"coords\"]\n",
    "    conf = inference[key][\"conf\"]\n",
    "#     print(pred_coords.shape)\n",
    "#     break\n",
    "\n",
    "\n",
    "    errs.append([key, neg_multi_log_likelihood(gt_coord, pred_coords, conf, avail)])\n",
    "    \n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "errs = np.array(errs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.405956"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(errs[:, 1].astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
